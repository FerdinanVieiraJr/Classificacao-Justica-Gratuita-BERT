{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMT/r4nsdr75DRZn8P4s+oG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerdinanVieiraJr/Classificacao-Justica-Gratuita-BERT/blob/main/gratuidade_BERTimbau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghr2-lRqN5ub"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import unicodedata\n",
        "\n",
        "NUM_AMOSTRAS = 50000\n",
        "\n",
        "# --- 1. FUN√á√ïES DE RU√çDO\n",
        "\n",
        "def remover_acentos(texto):\n",
        "    \"\"\"Transforma 'Declara√ß√£o' em 'Declaracao' (comum em sistemas antigos ou erro de usuario).\"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def erro_digitacao(texto, probabilidade=0.1):\n",
        "    \"\"\"\n",
        "    Simula erros comuns: troca de letra vizinha, supress√£o ou duplica√ß√£o.\n",
        "    A probabilidade controla o qu√£o 'analfabeto' o texto fica.\n",
        "    \"\"\"\n",
        "    if random.random() > probabilidade:\n",
        "        return texto\n",
        "\n",
        "    chars = list(texto)\n",
        "    n = len(chars)\n",
        "    if n < 3: return texto\n",
        "\n",
        "    idx = random.randint(0, n-2)\n",
        "    tipo_erro = random.choice(['troca', 'delete', 'duplica'])\n",
        "\n",
        "    if tipo_erro == 'troca':\n",
        "        # Troca caracteres adjacentes (ex: \"randa\" em vez de \"renda\")\n",
        "        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]\n",
        "    elif tipo_erro == 'delete':\n",
        "        # Comeu uma letra (ex: \"rend\" em vez de \"renda\")\n",
        "        del chars[idx]\n",
        "    elif tipo_erro == 'duplica':\n",
        "        # Tecla presa (ex: \"rendaa\")\n",
        "        chars.insert(idx, chars[idx])\n",
        "\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def variar_caixa(texto):\n",
        "    \"\"\"Varia entre: tudo minusculo, CAPITALIZADO, Primeira Maiuscula.\"\"\"\n",
        "    opcao = random.choice(['lower', 'title', 'upper', 'original'])\n",
        "    if opcao == 'lower': return texto.lower()\n",
        "    if opcao == 'upper': return texto.upper()\n",
        "    if opcao == 'title': return texto.title()\n",
        "    return texto\n",
        "\n",
        "# --- 2. VARIA√á√ïES DE SIN√îNIMOS (Data Augmentation Sem√¢ntico) ---\n",
        "# Vamos criar templates em vez de frases fixas para aumentar a diversidade.\n",
        "\n",
        "sujeitos = [\"A parte autora\", \"O requerente\", \"O autor\", \"A demandante\", \"O suplicante\", \"O promovente\"]\n",
        "verbos_ter = [\"possui\", \"tem\", \"det√©m\", \"aufere\", \"apresenta\"]\n",
        "verbos_declarar = [\"Declara\", \"Afirma\", \"Diz\", \"Informa\", \"Atesta\"]\n",
        "\n",
        "def gerar_frase_adversaria():\n",
        "    # Sorteia um template de cen√°rio\n",
        "    cenario = random.choice(['renda', 'bens', 'condicoes', 'veiculo', 'trabalho', 'isencao'])\n",
        "\n",
        "    sujeito = random.choice(sujeitos)\n",
        "    verbo_ter = random.choice(verbos_ter)\n",
        "    verbo_decl = random.choice(verbos_declarar)\n",
        "\n",
        "    if cenario == 'renda':\n",
        "        # Par 1: Renda\n",
        "        if random.random() > 0.5:\n",
        "            return f\"{sujeito} N√ÉO {verbo_ter} renda fixa.\", 1\n",
        "        else:\n",
        "            return f\"{sujeito} {verbo_ter} renda fixa.\", 0\n",
        "\n",
        "    elif cenario == 'bens':\n",
        "        # Par 2: Bens\n",
        "        if random.random() > 0.5:\n",
        "            return f\"{sujeito} N√ÉO {verbo_ter} bens declarados.\", 1\n",
        "        else:\n",
        "            return f\"{sujeito} {verbo_ter} bens declarados.\", 0\n",
        "\n",
        "    elif cenario == 'condicoes':\n",
        "        # Par 3: Condi√ß√µes\n",
        "        if random.random() > 0.5:\n",
        "            return f\"{verbo_decl} que N√ÉO {verbo_ter} condi√ß√µes de pagar as custas.\", 1\n",
        "        else:\n",
        "            return f\"{verbo_decl} que {verbo_ter} condi√ß√µes de pagar as custas.\", 0\n",
        "\n",
        "    elif cenario == 'veiculo':\n",
        "        # Par 4: Ve√≠culo (Contexto dif√≠cil)\n",
        "        # Aqui mantemos mais fixo pois a mudan√ßa de uma palavra quebra o sentido adversarial\n",
        "        if random.random() > 0.5:\n",
        "            return f\"{sujeito} vive de aluguel de ve√≠culo e trabalha como motorista.\", 1\n",
        "        else:\n",
        "            return f\"{sujeito} vive de renda de aluguel de ve√≠culos sendo dono de locadora.\", 0\n",
        "\n",
        "    elif cenario == 'trabalho':\n",
        "        # Par 5: Trabalho\n",
        "        if random.random() > 0.5:\n",
        "            return \"Est√° sem trabalho formal atualmente.\", 1\n",
        "        else:\n",
        "            return \"Est√° com trabalho formal atualmente.\", 0\n",
        "\n",
        "    elif cenario == 'isencao':\n",
        "        # Par 6: Isen√ß√£o\n",
        "        if random.random() > 0.5:\n",
        "            return \"√â isento de declara√ß√£o de imposto de renda.\", 1\n",
        "        else:\n",
        "            return \"N√£o √© isento de declara√ß√£o de imposto de renda.\", 0\n",
        "\n",
        "# --- 3. GERA√á√ÉO EM MASSA ---\n",
        "\n",
        "data = []\n",
        "\n",
        "print(f\"Gerando {NUM_AMOSTRAS} amostras com ru√≠do...\")\n",
        "\n",
        "for _ in range(NUM_AMOSTRAS):\n",
        "    frase_base, rotulo = gerar_frase_adversaria()\n",
        "\n",
        "    # Aplicar Pipeline de Ru√≠do (Probabil√≠stico)\n",
        "\n",
        "    # 1. Talvez remover acentos (30% de chance)\n",
        "    if random.random() < 0.3:\n",
        "        frase_base = remover_acentos(frase_base)\n",
        "\n",
        "    # 2. Varia√ß√£o de Caixa (Maiusc/Minusc)\n",
        "    frase_base = variar_caixa(frase_base)\n",
        "\n",
        "    # 3. Erro de digita√ß√£o (15% de chance de ter algum erro na frase)\n",
        "    if random.random() < 0.15:\n",
        "        frase_base = erro_digitacao(frase_base)\n",
        "\n",
        "    data.append([frase_base, rotulo])\n",
        "\n",
        "# Criar DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"texto_peticao\", \"rotulo\"])\n",
        "\n",
        "# Embaralhar\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Salvar\n",
        "nome_arquivo = \"dataset_justica_gratuita_50k_noisy.csv\"\n",
        "df.to_csv(nome_arquivo, index=False)\n",
        "\n",
        "print(f\"‚úÖ Arquivo '{nome_arquivo}' criado com sucesso!\")\n",
        "print(\"\\n--- Exemplos de Frases Geradas (Note a 'sujeira') ---\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Carregar o NOVO dataset ruidoso (50k amostras)\n",
        "arquivo = \"dataset_justica_gratuita_50k_noisy.csv\"\n",
        "print(f\"Carregando {arquivo}...\")\n",
        "df = pd.read_csv(arquivo)\n",
        "\n",
        "# Verificando se h√° nulos gerados pelo ru√≠do (seguran√ßa)\n",
        "df = df.dropna(subset=['texto_peticao'])\n",
        "\n",
        "# 2. Split (Treino e Teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['texto_peticao'],\n",
        "    df['rotulo'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. O Baseline: Bag of Words\n",
        "# OBS: O CountVectorizer por padr√£o faz lowercase=True.\n",
        "# Mas ele N√ÉO remove acentos por padr√£o. Ent√£o \"n√£o\" e \"nao\" ser√£o duas palavras diferentes!\n",
        "# Isso vai diluir a for√ßa do modelo.\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Tamanho do Vocabul√°rio aprendido: {len(vectorizer.get_feature_names_out())} palavras\")\n",
        "print(\"(Note como o vocabul√°rio explodiu devido aos erros de digita√ß√£o gerados)\")\n",
        "\n",
        "# 4. Modelo Linear\n",
        "# Aumentado o max_iter para 1000 pois com 50k dados ruidosos a converg√™ncia √© mais dif√≠cil\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "print(\"Treinando o modelo (pode demorar alguns segundos)...\")\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 5. Avalia√ß√£o\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"\\n--- VOCABUL√ÅRIO APRENDIDO (Top features) ---\")\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "coefs = model.coef_[0]\n",
        "\n",
        "# Vamos ver o que o modelo considera \"Rico\" (Classe 0)\n",
        "print(\"\\nPalavras que puxam para INDEFERIDO (Rico):\")\n",
        "for i in coefs.argsort()[:10]:\n",
        "    print(f\"'{feature_names[i]}' -> Peso: {coefs[i]:.4f}\")\n",
        "\n",
        "# Vamos ver o que o modelo considera \"Pobre\" (Classe 1)\n",
        "print(\"\\nPalavras que puxam para GRATUIDADE (Pobre):\")\n",
        "for i in coefs.argsort()[-10:]:\n",
        "    print(f\"'{feature_names[i]}' -> Peso: {coefs[i]:.4f}\")\n",
        "\n",
        "print(\"\\n--- RELAT√ìRIO DE CLASSIFICA√á√ÉO ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 6. Teste de Fogo (Com varia√ß√µes ruidosas)\n",
        "print(\"\\n--- TESTE DE FOGO (O Baseline sobrevive ao ru√≠do?) ---\")\n",
        "testes = [\n",
        "    # Casos Cl√°ssicos (Limpos)\n",
        "    \"A parte autora n√£o possui renda fixa\",    # Esperado: 1\n",
        "    \"A parte autora possui renda fixa\",        # Esperado: 0\n",
        "\n",
        "    # A Armadilha do Contexto (Baseline costuma errar aqui)\n",
        "    \"N√£o √© isento de declara√ß√£o de imposto\",   # Esperado: 0 (Mas tem 'n√£o')\n",
        "    \"O autor vive de aluguel de ve√≠culo\",      # Esperado: 1 (Motorista), mas tem 'aluguel' (Rico)\n",
        "\n",
        "    # Casos \"Sujos\" (Simulando o Dataset novo)\n",
        "    \"a parte autora nao possui renda\",         # Sem acento (o modelo aprendeu 'nao' ou s√≥ 'n√£o'?)\n",
        "    \"DECLARA QUE TEM CONDICOES DE PAGAR\",      # Uppercase e sem cedilha (Esperado: 0)\n",
        "    \"o requerente nao tm bens declarados\"      # Typo em 'tem' -> 'tm'\n",
        "]\n",
        "\n",
        "vec_testes = vectorizer.transform(testes)\n",
        "preds = model.predict(vec_testes)\n",
        "\n",
        "for texto, pred in zip(testes, preds):\n",
        "    label = \"GRATUIDADE (1)\" if pred == 1 else \"INDEFERIDO (0)\"\n",
        "    print(f\"Frase: '{texto}'\")\n",
        "    print(f\" -> Predi√ß√£o: {label}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "mEuJjGQePW6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers seaborn"
      ],
      "metadata": {
        "id": "gGnueqEBbEZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, create_optimizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CARREGAMENTO DOS DADOS\n",
        "# ==============================================================================\n",
        "print(\"‚öôÔ∏è Configurando ambiente...\")\n",
        "\n",
        "# Carregar Dataset\n",
        "arquivo = \"dataset_justica_gratuita_50k_noisy.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(arquivo)\n",
        "    df = df.dropna(subset=['texto_peticao', 'rotulo'])\n",
        "    textos = df[\"texto_peticao\"].astype(str).tolist()\n",
        "    rotulos = df[\"rotulo\"].tolist()\n",
        "    print(f\"‚úÖ Dados carregados: {len(df)} amostras.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Erro: Arquivo CSV n√£o encontrado. Rode o gerador de dados primeiro.\")\n",
        "    exit()\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textos, rotulos, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TOKENIZA√á√ÉO (BERTimbau)\n",
        "# ==============================================================================\n",
        "MODELO_NOME = \"neuralmind/bert-base-portuguese-cased\"\n",
        "print(f\"üáßüá∑ Carregando Tokenizer ({MODELO_NOME})...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODELO_NOME)\n",
        "\n",
        "def tokenizar_dados(textos, max_len=128):\n",
        "    return tokenizer(\n",
        "        textos,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "print(\"üîÑ Tokenizando textos...\")\n",
        "X_train_tokens = dict(tokenizar_dados(X_train))\n",
        "X_test_tokens = dict(tokenizar_dados(X_test))\n",
        "\n",
        "y_train_array = np.array(y_train)\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CRIA√á√ÉO DO MODELO COM OTIMIZADOR SEGURO\n",
        "# ==============================================================================\n",
        "print(\"üèóÔ∏è Construindo o modelo...\")\n",
        "\n",
        "# Configura√ß√µes de Treino\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "batches_per_epoch = len(X_train) // BATCH_SIZE\n",
        "total_train_steps = int(batches_per_epoch * EPOCHS)\n",
        "\n",
        "# --- A CORRE√á√ÉO M√ÅGICA AQUI ---\n",
        "# Usei o create_optimizer do Transformers para evitar o erro de vers√£o do Keras\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_train_steps=total_train_steps,\n",
        "    num_warmup_steps=0,\n",
        "    weight_decay_rate=0.01\n",
        ")\n",
        "\n",
        "# Carrega o modelo (convertendo pesos PyTorch -> TF automaticamente)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    MODELO_NOME,\n",
        "    num_labels=2,\n",
        "    from_pt=True\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'])\n",
        "# Nota: N√£o precisamos passar 'loss' aqui, o modelo Hugging Face calcula internamente\n",
        "\n",
        "print(\"\\nüöÄ INICIANDO TREINAMENTO (3 √âpocas)...\")\n",
        "history = model.fit(\n",
        "    X_train_tokens,\n",
        "    y_train_array,\n",
        "    validation_data=(X_test_tokens, y_test_array),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. AVALIA√á√ÉO E GR√ÅFICOS\n",
        "# ==============================================================================\n",
        "print(\"\\nüìä Avaliando performance final...\")\n",
        "preds_logits = model.predict(X_test_tokens).logits\n",
        "y_pred_bert = np.argmax(preds_logits, axis=1)\n",
        "acc_final_bert = np.mean(y_pred_bert == y_test_array) * 100\n",
        "print(f\"Acur√°cia Final BERTimbau: {acc_final_bert:.2f}%\")\n",
        "\n",
        "# --- GR√ÅFICO 1: COMPARATIVO ---\n",
        "def plotar_comparacao_final():\n",
        "    print(\"\\nüìà Gerando Gr√°fico Comparativo...\")\n",
        "    modelos = ['Baseline (BoW)', 'BERTimbau (Ours)']\n",
        "    acuracias = [83.0, acc_final_bert]\n",
        "    cores = ['#e74c3c', '#2ecc71']\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    ax = sns.barplot(x=modelos, y=acuracias, palette=cores, hue=modelos, legend=False)\n",
        "    plt.ylim(0, 115)\n",
        "    plt.title('Comparativo de Acur√°cia: Tradicional vs Deep Learning', fontsize=15)\n",
        "    plt.ylabel('Acur√°cia (%)', fontsize=12)\n",
        "    for i, v in enumerate(acuracias):\n",
        "        ax.text(i, v + 2, f\"{v:.2f}%\", ha='center', fontsize=14, fontweight='bold')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "plotar_comparacao_final()\n",
        "\n",
        "# --- GR√ÅFICO 2: MATRIZ DE CONFUS√ÉO ---\n",
        "print(\"\\nüîç Gerando Matriz de Confus√£o...\")\n",
        "cm = confusion_matrix(y_test_array, y_pred_bert)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Pred: Rico (0)', 'Pred: Pobre (1)'],\n",
        "            yticklabels=['Real: Rico (0)', 'Real: Pobre (1)'])\n",
        "plt.title('Matriz de Confus√£o - BERTimbau', fontsize=15)\n",
        "plt.ylabel('Classe Verdadeira', fontsize=12)\n",
        "plt.xlabel('Predi√ß√£o do Modelo', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TABELA QUALITATIVA\n",
        "# ==============================================================================\n",
        "print(\"\\nüî• TESTE DE FOGO üî•\")\n",
        "frases_adversarias = [\n",
        "    \"A parte autora n√£o possui renda fixa\",\n",
        "    \"N√£o √© isento de declara√ß√£o de imposto de renda\",\n",
        "    \"a parte autora nao pssui renda fixa\",\n",
        "    \"O autor vive de aluguel de ve√≠culo (motorista de app)\",\n",
        "    \"DECLARA QUE TEM CONDICOES DE PAGAR AS CUSTAS\"\n",
        "]\n",
        "inputs_fogo = dict(tokenizar_dados(frases_adversarias))\n",
        "logits_fogo = model.predict(inputs_fogo).logits\n",
        "preds_fogo = np.argmax(logits_fogo, axis=1)\n",
        "\n",
        "print(\"\\n| Frase Advers√°ria (Input) | Baseline (BoW) | BERTimbau (Ours) | An√°lise |\")\n",
        "print(\"| :--- | :---: | :---: | :--- |\")\n",
        "respostas_baseline = [\"‚úÖ Pobre\", \"‚ùå Pobre\", \"‚ùå (Ignorou)\", \"‚ùå Pobre\", \"‚ùå Pobre\"]\n",
        "analises = [\n",
        "    \"Baseline acertou por sorte.\",\n",
        "    \"BERT entendeu a nega√ß√£o l√≥gica da isen√ß√£o.\",\n",
        "    \"BERT corrigiu erros de digita√ß√£o (pssui).\",\n",
        "    \"BERT analisou contexto sem√¢ntico de aluguel.\",\n",
        "    \"BERT entendeu a afirma√ß√£o positiva de pagamento.\"\n",
        "]\n",
        "\n",
        "for i, frase in enumerate(frases_adversarias):\n",
        "    pred_bert = preds_fogo[i]\n",
        "    icon_bert = \"‚úÖ\" if (pred_bert == 1 and i in [0, 2]) or (pred_bert == 0 and i in [1, 3, 4]) else \"‚ö†Ô∏è\"\n",
        "    label_bert = \"Pobre\" if pred_bert == 1 else \"Rico\"\n",
        "    print(f\"| \\\"{frase}\\\" | {respostas_baseline[i]} | {icon_bert} {label_bert} | {analises[i]} |\")\n",
        "\n",
        "# Salvar\n",
        "print(\"\\nüíæ Salvando modelo BERTimbau...\")\n",
        "model.save_pretrained(\"modelo_bertimbau_final\")\n",
        "tokenizer.save_pretrained(\"modelo_bertimbau_final\")\n",
        "print(\"‚úÖ Salvo com sucesso!\")"
      ],
      "metadata": {
        "id": "MiM2FqDWcH7t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}